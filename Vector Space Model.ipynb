{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE8Em2BPL0Ww",
        "outputId": "55326b3f-5c9a-4a70-cd8b-39665a844cd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Eu9yd0nZL4gz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "s_ydjrR1MCmV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "107db33a-2854-4ccd-9361-527d2b5cc8d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS = stopwords.words('english')\n",
        "LEMMATIZER = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "KbH9zpE9MEs-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text_files(folder_path):\n",
        "    data = []\n",
        "    filenames = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            filenames.append(filename)\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                data.append(file.read())\n",
        "    return filenames, data"
      ],
      "metadata": {
        "id": "l8XLPquZMSCM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "    tokens = word_tokenize(text)\n",
        "    cleaned_tokens = [LEMMATIZER.lemmatize(word) for word in tokens if word not in STOPWORDS]\n",
        "    return cleaned_tokens\n"
      ],
      "metadata": {
        "id": "678U9IOIMTt6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def term_frequency(term, document):\n",
        "    return document.count(term) / len(document)"
      ],
      "metadata": {
        "id": "UZwu3_LaMdO2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inverse_document_frequency(term, all_documents):\n",
        "    num_docs_containing_term = sum(1 for doc in all_documents if term in doc)\n",
        "    return math.log(len(all_documents) / (1 + num_docs_containing_term))"
      ],
      "metadata": {
        "id": "8Qr93mRHMexm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tfidf(document, all_documents, vocab):\n",
        "    tfidf_vector = []\n",
        "    for term in vocab:\n",
        "        tf = term_frequency(term, document)\n",
        "        idf = inverse_document_frequency(term, all_documents)\n",
        "        tfidf_vector.append(tf * idf)\n",
        "    return np.array(tfidf_vector)"
      ],
      "metadata": {
        "id": "JvgeD0WXMgQG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2)"
      ],
      "metadata": {
        "id": "PgdPrx6aMhsu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    folder_path = '/content/drive/MyDrive/dataset/documents'\n",
        "\n",
        "    filenames, documents = load_text_files(folder_path)\n",
        "\n",
        "    tokenized_docs = [clean_text(doc) for doc in documents]\n",
        "\n",
        "    vocab = sorted(set([word for doc in tokenized_docs for word in doc]))\n",
        "\n",
        "    doc_tfidf_vectors = [compute_tfidf(doc, tokenized_docs, vocab) for doc in tokenized_docs]\n",
        "\n",
        "    queries = [\"Harris' key economic plans\",\n",
        "               \"Trump immigration plan this time around\",\n",
        "               \"the big moments from the Harris vs. Trump debate\"\n",
        "               ]\n",
        "    cleaned_queries = [clean_text(query) for query in queries]\n",
        "\n",
        "    query_tfidf_vectors = [compute_tfidf(query, tokenized_docs, vocab) for query in cleaned_queries]\n",
        "\n",
        "    cosine_similarities = []\n",
        "    for query_vector in query_tfidf_vectors:\n",
        "        similarities = [cosine_similarity(query_vector, doc_vector) for doc_vector in doc_tfidf_vectors]\n",
        "        cosine_similarities.append(similarities)\n",
        "\n",
        "    output_file = 'output.txt'\n",
        "    with open(output_file, 'w') as file:\n",
        "        for i, query in enumerate(queries):\n",
        "            file.write(\"\\n\")\n",
        "            file.write(\"---------------------------------------------------------\\n\")\n",
        "            file.write(f\"Cosine similarities for query \\\"{query}\\\":\\n\")\n",
        "            file.write(\"-------------------------------------------------------------\\n\")\n",
        "            doc_similarities = [(filenames[j], cosine_similarities[i][j]) for j in range(len(documents))]\n",
        "            sorted_similarities = sorted(doc_similarities, key=lambda x: x[1], reverse=True)\n",
        "            for filename, similarity in sorted_similarities:\n",
        "                file.write(f\"{filename}: {similarity:.4f}\\n\")\n",
        "\n",
        "    with open(output_file, 'r') as file:\n",
        "        print(file.read())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "r3GGRViGMi5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52bb2006-737c-4e44-962f-797e51632d3d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------------------------------------------------------\n",
            "Cosine similarities for query \"Harris' key economic plans\":\n",
            "-------------------------------------------------------------\n",
            "Trump-Harris debate sums up policy-light US election.txt: 0.0699\n",
            "US election latest.txt: 0.0304\n",
            "2024 US election Kamala Harris's transformation.txt: 0.0223\n",
            "The Harris–Trump debate showed US foreign policy matters in this election.txt: 0.0162\n",
            "Mounting North Korean threats await next US president.txt: 0.0153\n",
            "Arm the public with facts Microsoft billionaire fights US election disinformation.txt: 0.0098\n",
            "US election polls Who is ahead - Harris or Trump.txt: 0.0035\n",
            "Fears mount that election deniers could disrupt vote count in US swing.txt: 0.0013\n",
            "Pope Francis tells US Catholics to choose ‘lesser evil’ in coming election.txt: 0.0009\n",
            "State of the Union Germany's border politics and US election campaign fever.txt: 0.0000\n",
            "\n",
            "---------------------------------------------------------\n",
            "Cosine similarities for query \"Trump immigration plan this time around\":\n",
            "-------------------------------------------------------------\n",
            "Trump-Harris debate sums up policy-light US election.txt: 0.0617\n",
            "US election latest.txt: 0.0306\n",
            "State of the Union Germany's border politics and US election campaign fever.txt: 0.0239\n",
            "US election polls Who is ahead - Harris or Trump.txt: 0.0220\n",
            "2024 US election Kamala Harris's transformation.txt: 0.0098\n",
            "The Harris–Trump debate showed US foreign policy matters in this election.txt: 0.0082\n",
            "Arm the public with facts Microsoft billionaire fights US election disinformation.txt: 0.0068\n",
            "Mounting North Korean threats await next US president.txt: 0.0062\n",
            "Fears mount that election deniers could disrupt vote count in US swing.txt: 0.0032\n",
            "Pope Francis tells US Catholics to choose ‘lesser evil’ in coming election.txt: 0.0013\n",
            "\n",
            "---------------------------------------------------------\n",
            "Cosine similarities for query \"the big moments from the Harris vs. Trump debate\":\n",
            "-------------------------------------------------------------\n",
            "US election polls Who is ahead - Harris or Trump.txt: 0.0257\n",
            "Pope Francis tells US Catholics to choose ‘lesser evil’ in coming election.txt: 0.0196\n",
            "Fears mount that election deniers could disrupt vote count in US swing.txt: 0.0099\n",
            "Arm the public with facts Microsoft billionaire fights US election disinformation.txt: 0.0090\n",
            "2024 US election Kamala Harris's transformation.txt: 0.0078\n",
            "Trump-Harris debate sums up policy-light US election.txt: 0.0071\n",
            "US election latest.txt: 0.0047\n",
            "The Harris–Trump debate showed US foreign policy matters in this election.txt: 0.0032\n",
            "Mounting North Korean threats await next US president.txt: 0.0025\n",
            "State of the Union Germany's border politics and US election campaign fever.txt: 0.0003\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fZTyPjK_NKZf"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}